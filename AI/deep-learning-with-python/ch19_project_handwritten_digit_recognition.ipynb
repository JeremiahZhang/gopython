{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "D:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:34: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(784, activation=\"relu\", kernel_initializer=\"normal\", input_dim=784)`\n",
      "D:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:35: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(10, activation=\"softmax\", kernel_initializer=\"normal\")`\n",
      "D:\\ProgramData\\Anaconda2\\lib\\site-packages\\keras\\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "5s - loss: 0.2794 - acc: 0.9206 - val_loss: 0.1396 - val_acc: 0.9585\n",
      "Epoch 2/10\n",
      "4s - loss: 0.1102 - acc: 0.9679 - val_loss: 0.0896 - val_acc: 0.9727\n",
      "Epoch 3/10\n",
      "4s - loss: 0.0702 - acc: 0.9802 - val_loss: 0.0794 - val_acc: 0.9763\n",
      "Epoch 4/10\n",
      "4s - loss: 0.0491 - acc: 0.9859 - val_loss: 0.0733 - val_acc: 0.9786\n",
      "Epoch 5/10\n",
      "4s - loss: 0.0358 - acc: 0.9901 - val_loss: 0.0670 - val_acc: 0.9797\n",
      "Epoch 6/10\n",
      "4s - loss: 0.0261 - acc: 0.9929 - val_loss: 0.0645 - val_acc: 0.9812\n",
      "Epoch 7/10\n",
      "4s - loss: 0.0196 - acc: 0.9956 - val_loss: 0.0585 - val_acc: 0.9821\n",
      "Epoch 8/10\n",
      "4s - loss: 0.0127 - acc: 0.9975 - val_loss: 0.0608 - val_acc: 0.9817\n",
      "Epoch 9/10\n",
      "4s - loss: 0.0100 - acc: 0.9979 - val_loss: 0.0585 - val_acc: 0.9815\n",
      "Epoch 10/10\n",
      "4s - loss: 0.0077 - acc: 0.9987 - val_loss: 0.0593 - val_acc: 0.9817\n",
      "Baseline Error: 1.83%\n"
     ]
    }
   ],
   "source": [
    "# Baseline MLP for MNIST dataset\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Random seed\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 28*28 pixels to 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "# Normailize input from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# One hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "# Define baseline model\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, init='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, init='normal', activation='softmax'))\n",
    "    # Compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200,\n",
    "         verbose=2)\n",
    "# Final evaluation of the model\n",
    "# (X_test, y_test) 即用作 validation dataset 又用作 testing dataset 不好\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: {0:.2f}%\".format(100 - scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:42: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), padding=\"valid\", activation=\"relu\", input_shape=(1, 28, 28...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "40s - loss: 0.2351 - acc: 0.9336 - val_loss: 0.0796 - val_acc: 0.9761\n",
      "Epoch 2/10\n",
      "40s - loss: 0.0705 - acc: 0.9790 - val_loss: 0.0438 - val_acc: 0.9856\n",
      "Epoch 3/10\n",
      "42s - loss: 0.0494 - acc: 0.9848 - val_loss: 0.0389 - val_acc: 0.9877\n",
      "Epoch 4/10\n",
      "43s - loss: 0.0403 - acc: 0.9872 - val_loss: 0.0392 - val_acc: 0.9871\n",
      "Epoch 5/10\n",
      "41s - loss: 0.0328 - acc: 0.9899 - val_loss: 0.0346 - val_acc: 0.9880\n",
      "Epoch 6/10\n",
      "41s - loss: 0.0265 - acc: 0.9915 - val_loss: 0.0327 - val_acc: 0.9899\n",
      "Epoch 7/10\n",
      "41s - loss: 0.0221 - acc: 0.9928 - val_loss: 0.0341 - val_acc: 0.9876\n",
      "Epoch 8/10\n",
      "41s - loss: 0.0186 - acc: 0.9940 - val_loss: 0.0286 - val_acc: 0.9899\n",
      "Epoch 9/10\n",
      "42s - loss: 0.0162 - acc: 0.9950 - val_loss: 0.0338 - val_acc: 0.9889\n",
      "Epoch 10/10\n",
      "41s - loss: 0.0144 - acc: 0.9954 - val_loss: 0.0306 - val_acc: 0.9901\n",
      "Small CNN error: 0.99%\n"
     ]
    }
   ],
   "source": [
    "# a simple CNN\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "# Random seed\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][channels][width][height]\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "# Normailize input from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# One hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "def simple_cnn_model():\n",
    "    model = Sequential()\n",
    "    # 1st-hidden layers: convolutional layer: Convolution2D\n",
    "    model.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=(1, 28, 28),\n",
    "                           activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "small_cnn_model = simple_cnn_model()\n",
    "# Fit\n",
    "small_cnn_model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10,\n",
    "                    batch_size=200, verbose=2)\n",
    "# Final evaluation\n",
    "scores = small_cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Small CNN error: {0:.2f}%\".format(100 - scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:46: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(30, (5, 5), activation=\"relu\", input_shape=(1, 28, 28...)`\n",
      "D:\\ProgramData\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:48: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(15, (3, 3), activation=\"relu\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "39s - loss: 0.3783 - acc: 0.8799 - val_loss: 0.0797 - val_acc: 0.9754\n",
      "Epoch 2/10\n",
      "40s - loss: 0.0925 - acc: 0.9712 - val_loss: 0.0520 - val_acc: 0.9828\n",
      "Epoch 3/10\n",
      "40s - loss: 0.0687 - acc: 0.9783 - val_loss: 0.0373 - val_acc: 0.9883\n",
      "Epoch 4/10\n",
      "41s - loss: 0.0562 - acc: 0.9825 - val_loss: 0.0330 - val_acc: 0.9885\n",
      "Epoch 5/10\n",
      "43s - loss: 0.0473 - acc: 0.9853 - val_loss: 0.0328 - val_acc: 0.9896\n",
      "Epoch 6/10\n",
      "40s - loss: 0.0417 - acc: 0.9870 - val_loss: 0.0285 - val_acc: 0.9901\n",
      "Epoch 7/10\n",
      "41s - loss: 0.0383 - acc: 0.9877 - val_loss: 0.0295 - val_acc: 0.9899\n",
      "Epoch 8/10\n",
      "42s - loss: 0.0341 - acc: 0.9887 - val_loss: 0.0256 - val_acc: 0.9918\n",
      "Epoch 9/10\n",
      "42s - loss: 0.0306 - acc: 0.9902 - val_loss: 0.0222 - val_acc: 0.9918\n",
      "Epoch 10/10\n",
      "41s - loss: 0.0281 - acc: 0.9911 - val_loss: 0.0255 - val_acc: 0.9920\n",
      "Larger CNN error: 0.80%\n"
     ]
    }
   ],
   "source": [
    "# Large CNN for MNIST Dataset\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# Random seed\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape to be [samples][pixels][width][height]\n",
    "# It's important\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "# Normailize input from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# One hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "# Define large model\n",
    "def large_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(30, 5, 5, input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(15, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "large_cnn_model = large_cnn_model()\n",
    "large_cnn_model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, \n",
    "                    batch_size=200, verbose=2)\n",
    "\n",
    "# Final evaluate\n",
    "scores = large_cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Larger CNN error: {0:.2f}%\".format(100 - scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Linear Regression\n",
    "\n",
    "## 1.Homework\n",
    "\n",
    "Learning from data [第二周作业](http://www.work.caltech.edu/homework/hw2.pdf)的5-7问题. 使用线性回归, 探索分类问题."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析问题\n",
    "\n",
    "题目简要:\n",
    "\n",
    "- 数据集:\n",
    "    - D: 输入空间 X: [-1, 1] x [-1, 1] 均匀分布产生.\n",
    "        - 每个输入样本特征 2个, 每个特征参数范围为 [-1, 1]\n",
    "        - 样本数目为 N\n",
    "- Target Function: f\n",
    "    - 随机生成一条线, 随机从 [-1, 1] x [-1, 1] 空间取两个点, 通过这两个随机点的直线作为 target function.\n",
    "    - f 一边的样本划分为 +1 类别, 在另一边的样本划分为 -1 类别. 自己设定.\n",
    "    - 数据集 D 中的样本 x_n 进行 类别划分. (这样就生成完成数据集 (X, Y))\n",
    "- 模型: 线性回归模型\n",
    "- 问题5:\n",
    "    - 试验次数 1000\n",
    "    - 每一次试验\n",
    "        - 数据集样本数目 N, 数据集按上面的数据集部分产生 \n",
    "        - 使用线性回归 找到 final function g, 评估 E{in} = 分类不准确率, 1000次试验取平均\n",
    "        - 哪一个和问题中给出的接近.\n",
    "- 问题6:\n",
    "    - 试验 1000次\n",
    "    - 每次试验\n",
    "        - 随机产生 1000个 新的点, \n",
    "        - 用这些点计算 out-of-sample error E{out}(g), 最终假设使用 问题5 中得到的 g\n",
    "      - 取平均值, 看和问题中给出的哪一个选项接近  \n",
    "- 问题7: \n",
    "    - 数据集样本数目 N=10\n",
    "    - 1000 次 run\n",
    "    - 每一次run\n",
    "        - 使用 是 linear regression 得到的权重, 作为 PLA 算法的初始权重\n",
    "        - 记录 PLA 的收敛迭代次数 \n",
    "    - 取 1000次运行 迭代次数的平均值, 看问题中给出的选项, 哪一个更接近.\n",
    "- 线性回归:\n",
    "    - [LFD第三章笔记:线性模型 · Anifacc](https://anifacc.github.io/machinelearning/learningfromdata/2017/09/14/lfd-ch03-linear-model/)\n",
    "    - w{lin} = (X^T X)^-1 X^T y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification & Design\n",
    "\n",
    "### 1 产生数据集D\n",
    "\n",
    "generalize_dataset()\n",
    "\n",
    "1. 产生 N 个样本xi 的 特征(2d [-1, 1] x [-1, 1])\n",
    "2. 产生 N 个样本xi 对应的标签 yi(根据目标函数f得到)\n",
    "\n",
    "伪代码:\n",
    "\n",
    "```\n",
    "1. Generalize N samples in 2d, x1 = (x11, x12), ..., xN = (xN1, xN2) in [-1, 1]x[-1, 1]\n",
    "2. Through target function get the true labels of the samples Y1, Y2, ..., YN\n",
    "```\n",
    "\n",
    "### 2 产生目标函数 \n",
    "\n",
    "target_f()\n",
    "\n",
    "- 流程\n",
    "    1. 随机产生两个数据样本, 2d, [-1, 1] x [-1, 1]\n",
    "    2. 得到 tagrget function f\n",
    "    3. 划分类别\n",
    "- 伪代码\n",
    "\n",
    "```\n",
    "    1. Generalize 2 points in 2 dimension [-1, 1] x [-1, 1]\n",
    "        - (x1, y1)\n",
    "        - (x2, y2)\n",
    "    2. Through the 2 points calculating slope\n",
    "        - slope = (y2 - y1) / (x2 - x1)\n",
    "        - f: y = slope * (x - x1) + y1\n",
    "    3. if yi > slope * (xi - x1) + y1:\n",
    "        - then the label of the sample (xi, yi) is +1 \n",
    "        + else:\n",
    "            - the label of the sample (xi, yi) is -1\n",
    "```\n",
    "\n",
    "### 3 Final Hypothesis g\n",
    "\n",
    "hypothesis_g()\n",
    "\n",
    "1. 使用线性回归 得到权重\n",
    "2. 得到 g\n",
    "\n",
    "线性回归伪代码:\n",
    "\n",
    "```\n",
    "1. X = [x1^T, x2^T, ..., xN^T]^ T\n",
    "2. w{lin} = (X^T X)^-1 X^T y\n",
    "3. g = w^T x\n",
    "```\n",
    "\n",
    "### 问题5\n",
    "\n",
    "流程\n",
    "\n",
    "- one experiment\n",
    "    1. Generalize dataset D, random data in 2d [-1, 1] x [-1, 1] space, x1, x2, ..., xN\n",
    "    2. Genearlize the labels y1, y2, ..., yN of N samples\n",
    "    3. Training data\n",
    "        - Linear regression : 权重\n",
    "        - get finanl hypothesis g\n",
    "        - sign(g)\n",
    "        - Calculate E{in}\n",
    "- Repeat 1000 experiments\n",
    "    - Calculate average in-sample error E[in] average\n",
    " \n",
    "伪代码:\n",
    "\n",
    "```\n",
    "for i = 1, 2, ..., N_expriment,\n",
    "    run experiment:\n",
    "    1. through generalize_dataset() get dataset D (N samples)\n",
    "    2. throuth target_f() get the labels of the dataset\n",
    "    3. linear regression get weights\n",
    "    4. through hypothesis_g() get predict label\n",
    "    5. calculate E{in}_i = [[g != y]] / N\n",
    "    6. sum_E =  sum(E_{in}_i)\n",
    "\n",
    "calculate average E_{in} = sum_E / N_expriment\n",
    "   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 生成数据集\n",
    "def generate_dataset(data_size):\n",
    "    # A List of lists \n",
    "    dataset = [[1., random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "                for i in range(data_size)]\n",
    "    return dataset\n",
    "\n",
    "def target_f(feature, point1, point2):\n",
    "    # feature: [1, x1, x2]\n",
    "    x, y = feature[1], feature[2]\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    slope = (y2 - y1) / (x2 - x1)\n",
    "    \n",
    "    return 1 if y > (slope * (x - x1) + y1) else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 假设集判断\n",
    "def hypothesis(feature, weights):\n",
    "    return np.sign(np.dot(feature, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(training_set, p1, p2):\n",
    "    data_matrix = np.array(training_set)\n",
    "    target_vector = np.array([[target_f(point, p1, p2)] for point in training_set])\n",
    "    \n",
    "    pinv = np.linalg.pinv(data_matrix)\n",
    "    \n",
    "    weights = np.dot(pinv, target_vector)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 测试集测试\n",
    "def test(testing_set, p1, p2, weights):\n",
    "    mismatches = 0\n",
    "    \n",
    "    for feature in testing_set:\n",
    "        if hypothesis(feature, weights) != target_f(feature, p1, p2):\n",
    "            mismatches += 1\n",
    "            \n",
    "    mismatches /= float(len(testing_set))\n",
    "    \n",
    "    return mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiments(train_size, test_size, num_exp):\n",
    "    training_size  = train_size\n",
    "    testing_size = test_size\n",
    "    \n",
    "    avg_err_in = 0\n",
    "    avg_err_out = 0\n",
    "    \n",
    "    for i in xrange(num_exp):\n",
    "        p1 = (random.uniform(-1, 1), random.uniform(-1, 1))\n",
    "        p2 = (random.uniform(-1, 1), random.uniform(-1, 1))\n",
    "        \n",
    "        training_set = generate_dataset(training_size)\n",
    "        testing_set = generate_dataset(testing_size)\n",
    "        \n",
    "        final_weights = train(training_set, p1, p2)\n",
    "        \n",
    "        err_in = test(training_set, p1, p2, final_weights)\n",
    "        avg_err_in += err_in\n",
    "        \n",
    "        err_out = test(testing_set, p1, p2, final_weights)\n",
    "        avg_err_out += err_out\n",
    "    \n",
    "    avg_err_in /= float(num_exp)\n",
    "    avg_err_out /= float(num_exp)\n",
    "    \n",
    "    return avg_iter, avg_err_in, avg_err_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pla(training_set, p1, p2, weights=None):\n",
    "    misclassified = []\n",
    "    iterations = 0\n",
    "    \n",
    "    if weights is None:\n",
    "        weights = np.array([[0., 0., 0.]]).T\n",
    "    \n",
    "    while True:\n",
    "        for feature in training_set:\n",
    "            true_label = target_f(feature, p1, p2)\n",
    "            # 尚未正确分类的样本判断\n",
    "            if hypothesis(feature, weights) != true_label:\n",
    "                misclassified.append((feature, true_label))\n",
    "            \n",
    "        if not misclassified:\n",
    "            break\n",
    "        else:\n",
    "            iterations += 1\n",
    "            # 随机从错误分类的样本中选择一个样本, 进行权重更新\n",
    "            x, y = random.choice(misclassified)\n",
    "            # update w(t+1) = w(t) + y*x*\n",
    "            adapt = np.array([x]).T * y\n",
    "            weights += adapt\n",
    "            misclassified = []\n",
    "           \n",
    "    return iterations, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of E_in: 0.03948\n",
      "The average of E_out: 0.048839\n"
     ]
    }
   ],
   "source": [
    "data_size = 100\n",
    "test_size = 1000\n",
    "\n",
    "num_expriment = 1000\n",
    "\n",
    "avg_iter, avg_err_in, avg_err_out = experiments(data_size, test_size, num_expriment)\n",
    "\n",
    "print(\"The average of E_in: {}\".format(avg_err_in))\n",
    "print(\"The average of E_out: {}\".format(avg_err_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 7\n",
    "def experiment2(train_size, test_size, num_exp):\n",
    "    training_size  = train_size\n",
    "    testing_size = test_size\n",
    "    \n",
    "    avg_iter = 0\n",
    "    avg_err_in = 0\n",
    "    avg_err_out = 0\n",
    "    \n",
    "    for i in xrange(num_exp):\n",
    "        p1 = (random.uniform(-1, 1), random.uniform(-1, 1))\n",
    "        p2 = (random.uniform(-1, 1), random.uniform(-1, 1))\n",
    "        \n",
    "        training_set = generate_dataset(training_size)\n",
    "        testing_set = generate_dataset(testing_size)\n",
    "        \n",
    "        # linear regression\n",
    "        final_weights_lr = train(training_set, p1, p2)\n",
    "        \n",
    "        # pla\n",
    "        iterations, final_weights_pla = pla(training_set, p1, p2, \n",
    "                                            weights=final_weights_lr)\n",
    "        avg_iter += iterations\n",
    "        \n",
    "        err_in = test(training_set, p1, p2, final_weights_pla)\n",
    "        avg_err_in += err_in\n",
    "        \n",
    "        err_out = test(testing_set, p1, p2, final_weights_pla)\n",
    "        avg_err_out += err_out\n",
    "\n",
    "    \n",
    "    avg_iter /= float(num_exp)\n",
    "    avg_err_in /= float(num_exp)\n",
    "    avg_err_out /= float(num_exp)\n",
    "    \n",
    "    return avg_iter, avg_err_in, avg_err_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(N=10): Average iterations: 4.285.\n",
      "The average of E_in: 0.0\n",
      "The average of E_out: 0.1008\n"
     ]
    }
   ],
   "source": [
    "data_size = 10\n",
    "test_size = 10\n",
    "\n",
    "num_exp = 1000\n",
    "\n",
    "avg_iter, avg_err_in, avg_err_out = experiment2(data_size, test_size, num_exp)\n",
    "\n",
    "print(\"Dataset(N={0}): Average iterations: {1}.\".format(data_size, avg_iter))\n",
    "print(\"The average of E_in: {}\".format(avg_err_in))\n",
    "print(\"The average of E_out: {}\".format(avg_err_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises of chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex 4.1 \n",
    "\n",
    "P121\n",
    "\n",
    "10 阶多项式 包含 2阶多项式的假设, 只要 2阶以外的假设集参数为0就得到2阶的假设集\n",
    "\n",
    "看来答案是对的: 可参考\n",
    "\n",
    "[SlidesLect12.dvi - SlidesLect12.pdf](http://www.cs.rpi.edu/~magdon/courses/LFD-Slides/SlidesLect12.pdf) 中的 Constraining The Model:H_10 vs H_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex 4.2\n",
    "\n",
    "experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex 4.3\n",
    "\n",
    "参考: [SlidesLect11.dvi - SlidesLect11.pdf](http://www.cs.rpi.edu/~magdon/courses/LFD-Slides/SlidesLect11.pdf) Case Study: 2nd vs 10th Order Polynomial Fit 部分\n",
    "\n",
    "for part (a)\n",
    "\n",
    "假设 H 固定, 增加 目标函数 f 复杂度, \n",
    "\n",
    "deterministic noise : $$g - f$$\n",
    "\n",
    "f 带噪声么???\n",
    "\n",
    "猜测: \n",
    "\n",
    "(不带噪声的)目标函数复杂度增加, det.noise 减小. overfitting 趋势是 lower\n",
    "\n",
    "for part (b)\n",
    "\n",
    "f 固定, 那么增加 H 的复杂度, det.noise 增加, overfiting 趋势 higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex4.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "不知如何证明\n",
    "\n",
    "参考: https://drive.google.com/file/d/0B1HdZnjykKw8OTNxRk82QzZud2M/view \n",
    "\n",
    "未有自己的解法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ex4.5\n",
    "\n",
    "> for (a)\n",
    "\n",
    "$$\\varGamma^T \\varGamma = I$$\n",
    "\n",
    "可得出:\n",
    "\n",
    "$$\\varGamma$$ 中的每一列 都是 单位正交列向量\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> for ((b))\n",
    "\n",
    "$$\\varGamma = \n",
    "\\begin{bmatrix} \n",
    "1 & 1 & \\cdots & 1_{q}  \\\\ \n",
    "0 & 0 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 0 \n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ex4.6 \n",
    "\n",
    "可参考:[Exercise 4.6 - LFD Book Forum](http://book.caltech.edu/bookforum/showthread.php?t=4667)\n",
    "\n",
    "soft-order constraint 一加, 假设集数目减少, var 减小程度, 大于 bias 增加程度.\n",
    "\n",
    "> Yes, the soft order constraint does not impact classification. Better regularize with the hard order constraint, or use the soft order constraint with the \"regression for classification\" algorithm.\n",
    "\n",
    "还得回去review 一下, 才能明白."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ex4.7 \n",
    "\n",
    "> for part a\n",
    "\n",
    "$$E_{val}(g^{-1}) = \\frac{1}{K} \\sum_{{\\bf x}_n \\in D_{val}} e(g^{-}({\\bf x}_n), y_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_{val}^2 = Var_{D_{val}}[E_{val}(g^{-})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_{val}^2 = {\\bf Var}_{D_{val}} \\left( \\frac{1}{K} \\sum_{{\\bf x}_n \\in D_{val}} e(g^{-}({\\bf x}_n), y_n) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_{val}^2 = \\sum_{{\\bf x}_n \\in D_{val}} \\left( {\\bf Var}_{{\\bf x}_n} \\left( \\frac{1}{K} e(g^{-}({\\bf x}_n), y_n)\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各 validation 集 相互独立, 则有\n",
    "\n",
    "$$\\sigma_{val}^2 = \\frac{1}{K^2} \\left( \\sum_{{\\bf x}_n \\in D_{val}} {\\bf Var}_{{\\bf x}_n}(e(g^{-}({\\bf x}_n), y_n)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_{val}^2 = \\frac{1}{K^2} \\cdot K \\cdot {\\bf Var}_{{\\bf x}_n}(e(g^{-}({\\bf x}_n), y_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_{val}^2 = \\frac{1}{K} \\sigma^2(g^{-})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> for part b\n",
    "\n",
    "$$\\sigma_{val}^2 = \\frac{1}{K} \\sigma^2(g^{-})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma^2(g^{-}) = \\frac{1}{K} {\\bf Var}_{{\\bf x}}[e(g^{-}({\\bf x}, y))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma_{val}^2 = \\frac{1}{K} P(1-P)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> for part c\n",
    "\n",
    "$$P(1-P) \\leq \\frac{1}{4}$$, 当且仅当 $$P=\\frac{1}{2}$$ 成立.\n",
    "\n",
    "所以 \n",
    "\n",
    "$$\\sigma_{val}^2 = \\frac{1}{K} P(1-P) \\leq \\frac{1}{4K}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> for part (d)\n",
    "\n",
    "The squared error is unbounded. \n",
    "\n",
    "那么 $$Var[E_{val}(g^{-})]$$ 也无法 bounded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> for part (E)\n",
    "\n",
    "see Figure 4.8 in the textbook\n",
    "\n",
    "K 越小, 则 $$\\sigma^2(g^{-1})$$ 越大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> for part (f)\n",
    "\n",
    "see Figure 4.8 , we can get the results. Since E_{val} is a unbias estimate of E_{out}\n",
    "\n",
    "better E_{out}, but not always better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the answers of other people.\n",
    "\n",
    "[Learning From Data – A Short Course: Exercise 4.7 – Nguyen Thuy Vy](http://www.vynguyen.net/2016/04/06/learning-from-data-a-short-course-exercise-4-7/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex4.8\n",
    "\n",
    "Yes. 看 E_m 的定义就知道."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ex4.9\n",
    "\n",
    "> (1)\n",
    "\n",
    "K 增大, N-K 训练样本数目就减小, $$E_{in}$$ 就差, 导致 g 也差, 从而 $$E_{val}(g^{-}_{m^{\\ast}})$$ 也变差(增大)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (2)\n",
    "\n",
    "K 增加, 训练样本数目 N-K 则减少.\n",
    "\n",
    "$$E_{out}(g^{-}_{m^{\\ast}})$$ 与 $$E_{val}(g^{-}_{m^{\\ast}})$$ 也变大, 越来越接近\n",
    "\n",
    "$$E_{out}(g^{-}_{m^{\\ast}}) \\leq E_{val}(g^{-}_{m^{\\ast}}) + O \\left( \\frac{\\sqrt{\\ln M}}{\\sqrt K} \\right)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex4.10\n",
    "\n",
    "> (a)\n",
    "\n",
    "K 小, $$g_m^{-} \\approx g_m$$, 但是 $$E_{val}$$ 就与 $$E_{out}$$ 差距大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面不知道该怎么解.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ex4.11\n",
    "\n",
    "The black curve lies above the red curve\n",
    "\n",
    "out_of_sample 误差 比 cv 大. VC维 边界."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Learning Algorithm\n",
    "\n",
    "## 1.Homeworks\n",
    "\n",
    "Learning from data [第一周作业](http://www.work.caltech.edu/homework/hw1.pdf) 的问题7-10部分, 使用感知器学习算法, 加深理解.\n",
    "\n",
    "要知道, 现在我们是在写程序完成作业. 这个小程序就是告诉计算机, 让它按我们的想法(程序)完成任务.\n",
    "\n",
    "题目简要, 每次试验\n",
    "\n",
    "- 数据集 D\n",
    "    - 输入空间 X: [-1, 1] x [-1, 1] 均匀分布产生.\n",
    "        - 每个输入样本特征 2个\n",
    "        - 样本数目为 N(下面的题目会给定)\n",
    "- Target function: f\n",
    "    - 随机生成一条线, 随机从 [-1, 1] x [-1, 1] 空间取两个点, 通过这两个随机点的直线作为 target function.\n",
    "    -  f 一边的样本划分为 +1 类别, 在另一边的样本划分为 -1 类别. 自己设定.\n",
    "    - 数据集 D 中的样本 x_n 进行 类别划分. (这样就生成完成数据集 (X, Y))\n",
    "- PLA: 感知器算法学习\n",
    "    - 利用PLA 得到 final function g\n",
    "    - 初始权重: w(0) 为 0. 起始时, 所有样本都是被错误分类的. 因为 sign(0) = 0 \n",
    "- 问题关注点:\n",
    "    - 多少次迭代之后, PLA 收敛\n",
    "    - f 与 g 之间的误差率. P[f(x) != g(x)], 训练集训练, 测试集测试, 才有这个误差率\n",
    " \n",
    "试验次数 1000 次, 然后去平均值.\n",
    "\n",
    "感知器算法 LFD 笔记: [LFD第三章笔记:线性模型 · Anifacc](https://anifacc.github.io/machinelearning/learningfromdata/2017/09/14/lfd-ch03-linear-model/)\n",
    "\n",
    "PLA 的流程:\n",
    "\n",
    "```\n",
    "# PLA\n",
    "\n",
    "1: Initialize at step t = 0 to w(0).\n",
    "\n",
    "2: for t = 0, 1, 2, ... do\n",
    "3:    the weight vector is w(t)\n",
    "4:    From training dataset(x_1, y_1), ...(x_N, y_N) pick any misclassified example.\n",
    "            Call the misclassified example (x*, y*)\n",
    "            sign(w(t)^T x*) != y*;\n",
    "\n",
    "5:    Update the weight: w(t+1) = w(t) + y*x*\n",
    "6:    t = t+1   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 生成数据集\n",
    "def generate_dataset(data_size):\n",
    "    # list of lists\n",
    "    dataset = [[1., random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "                for i in range(data_size)]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 目标函数值, 真实值\n",
    "def target_function(feature, piont_1, point_2): \n",
    "    # 样本特征:[1, x1, x2]\n",
    "    x, y = feature[1], feature[2]\n",
    "\n",
    "    x1, y1 = point_1\n",
    "    x2, y2 = point_2\n",
    "    slope = (y2- y1) / (x2 - x1)\n",
    "    # 过两点的直线为目标函数\n",
    "    # 在直线上的样本标记+1, 否则为 -1\n",
    "    return 1 if y > (slope * (x - x1) + y1) else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 假设集判断\n",
    "def hypothesis(feature, weights):\n",
    "    return np.sign(np.dot(feature, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train(training_set, point_1, point_2, weights=None):\n",
    "    misclassified = []\n",
    "    iterations = 0\n",
    "    \n",
    "    if weights is None:\n",
    "        weights = np.array([[0., 0., 0.]]).T\n",
    "    else:\n",
    "        weights = weights\n",
    "    \n",
    "    while True:\n",
    "        for feature in training_set:\n",
    "            true_label = target_function(feature, point_1, point_2)\n",
    "            \n",
    "            # 尚未正确分类的样本判断\n",
    "            if hypothesis(feature, weights) != true_label:\n",
    "                misclassified.append((feature, true_label))\n",
    "        \n",
    "        if not misclassified:\n",
    "            break\n",
    "        else:\n",
    "            iterations += 1\n",
    "            # 随机从错误分类的样本中选择一个样本, 进行权重更新\n",
    "            x, y = random.choice(misclassified)\n",
    "            # update w(t+1) = w(t) + y*x*\n",
    "            adapt = np.array([x]).T * y\n",
    "            weights += adapt\n",
    "            misclassified = []\n",
    "    \n",
    "    return iterations, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 测试集测试\n",
    "def test(testing_set, point_1, point_2, weights):\n",
    "    mismatches = 0\n",
    "    \n",
    "    for feature in testing_set:\n",
    "        if hypothesis(feature, weights) != target_function(feature, point_1, point_2):\n",
    "            mismatches += 1\n",
    "            \n",
    "    mismatches /= float(len(testing_set))\n",
    "    \n",
    "    return mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(N=10): Average iterations:9.423(average error: 0.1083),\n"
     ]
    }
   ],
   "source": [
    "# 试验1000次\n",
    "num = 1000\n",
    "\n",
    "training_size = 10\n",
    "testing_size = 10\n",
    "\n",
    "avg_iter = 0\n",
    "avg_err = 0\n",
    "\n",
    "for i in range(num):\n",
    "    point_1 = (random.uniform(-1, 1), random.uniform(-1, 1))\n",
    "    point_2 = (random.uniform(-1, 1), random.uniform(-1, 1))\n",
    "    \n",
    "    training_set = generate_dataset(training_size)\n",
    "    testing_set = generate_dataset(testing_size)\n",
    "    \n",
    "    iterations, final_weights = train(training_set, point_1, point_2)\n",
    "    avg_iter += iterations\n",
    "    \n",
    "    test_err = test(testing_set, point_1, point_2, final_weights)\n",
    "    avg_err += test_err\n",
    "    \n",
    "avg_iter /= float(num) \n",
    "avg_err /= float(num)  \n",
    "\n",
    "print(\"Dataset(N={0}): Average iterations:{1}(average error: {2}),\".format(\n",
    "            training_size, avg_iter, avg_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(data_size, run_number, weights=None):\n",
    "    training_size = testing_size = data_size\n",
    "    \n",
    "    avg_iter = 0\n",
    "    avg_err = 0\n",
    "    \n",
    "    for i in xrange(run_number):\n",
    "        # 每一次试验, 都要随机选择点, 构造目标函数 \n",
    "        # 这里一定要注意, 我在这里就 debug 了好久\n",
    "        point_1 = (random.uniform(-1, 1), random.uniform(-1, 1))\n",
    "        point_2 = (random.uniform(-1, 1), random.uniform(-1, 1))\n",
    "    \n",
    "        training_set = generate_dataset(training_size)\n",
    "        testing_set = generate_dataset(testing_size)\n",
    "        \n",
    "        \n",
    "        iterations, final_weights = train(training_set, point_1, point_2)\n",
    "        avg_iter += iterations\n",
    "        \n",
    "        test_err = test(testing_set, point_1, point_2, final_weights)\n",
    "        avg_err += test_err\n",
    "        \n",
    "    avg_iter /= float(run_number) \n",
    "    avg_err /= float(run_number)\n",
    "        \n",
    "    return avg_iter, avg_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(N=10): Average iterations:7.583(average error: 0.1094),\n",
      "Dataset(N=100): Average iterations:100.401(average error: 0.01432),\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_size = [10, 100]\n",
    "    run_number = 1000\n",
    "    \n",
    "    for i in data_size:\n",
    "        avg_iter, avg_err = run_experiment(i, run_number)\n",
    "        print(\"Dataset(N={0}): Average iterations:{1}(average error: {2}),\".format(\n",
    "            i, avg_iter, avg_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题:\n",
    "\n",
    "我将上述代码直接放在 python 脚本中, 运行, 就出错.\n",
    "\n",
    "```\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 生成数据集\n",
    "def generate_dataset(data_size):\n",
    "    # list of lists\n",
    "    dataset = [[1., random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "                for i in range(data_size)]\n",
    "    return dataset\n",
    "\n",
    "# 目标函数值, 真实值\n",
    "def target_function(feature, piont_1, point_2):\n",
    "    # 样本特征:[1, x1, x2]\n",
    "    x, y = feature[1], feature[2]\n",
    "\n",
    "    x1, y1 = point_1\n",
    "    x2, y2 = point_2\n",
    "    slope = (y2- y1) / (x2 - x1)\n",
    "    # 过两点的直线为目标函数\n",
    "    # 在直线上的样本标记+1, 否则为 -1\n",
    "    return 1 if y > (slope * (x - x1) + y1) else -1\n",
    "\n",
    "# 假设集判断\n",
    "def hypothesis(feature, weights):\n",
    "    return np.sign(np.dot(feature, weights))\n",
    "\n",
    "# 模型训练\n",
    "def train(training_set, point_1, point_2, weights=None):\n",
    "    misclassified = []\n",
    "    iterations = 0\n",
    "\n",
    "    if weights is None:\n",
    "        weights = np.array([[0., 0., 0.]]).T\n",
    "    else:\n",
    "        weights = weights\n",
    "\n",
    "    while True:\n",
    "        for feature in training_set:\n",
    "            true_label = target_function(feature, point_1, point_2)\n",
    "\n",
    "            # 尚未正确分类的样本判断\n",
    "            if hypothesis(feature, weights) != true_label:\n",
    "                misclassified.append((feature, true_label))\n",
    "\n",
    "        if not misclassified:\n",
    "            break\n",
    "        else:\n",
    "            iterations += 1\n",
    "            # 随机从错误分类的样本中选择一个样本, 进行权重更新\n",
    "            x, y = random.choice(misclassified)\n",
    "            # update w(t+1) = w(t) + y*x*\n",
    "            adapt = np.array([x]).T * y\n",
    "            weights += adapt\n",
    "            misclassified = []\n",
    "\n",
    "    return iterations, weights\n",
    "\n",
    "# 测试集测试\n",
    "def test(testing_set, point_1, point_2, weights):\n",
    "    mismatches = 0\n",
    "\n",
    "    for feature in testing_set:\n",
    "        if hypothesis(feature, weights) != target_function(feature, point_1, point_2):\n",
    "            mismatches += 1\n",
    "\n",
    "    mismatches /= float(len(testing_set))\n",
    "\n",
    "    return mismatches\n",
    "\n",
    "def run_experiment(data_size, run_number, weights=None):\n",
    "    training_size = testing_size = data_size\n",
    "\n",
    "    avg_iter = 0\n",
    "    avg_err = 0\n",
    "\n",
    "    for i in range(run_number):\n",
    "        point_1 = (random.uniform(-1, 1), random.uniform(-1, 1))\n",
    "        point_2 = (random.uniform(-1, 1), random.uniform(-1, 1))\n",
    "\n",
    "        training_set = generate_dataset(training_size)\n",
    "        testing_set = generate_dataset(testing_size)\n",
    "\n",
    "        iterations, final_weights = train(training_set, point_1, point_2)\n",
    "        avg_iter += iterations\n",
    "\n",
    "        test_err = test(testing_set, point_1, point_2, final_weights)\n",
    "        avg_err += test_err\n",
    "\n",
    "    avg_iter /= float(run_number)\n",
    "    avg_err /= float(run_number)\n",
    "\n",
    "    return avg_iter, avg_err\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_size = [10, 100]\n",
    "    run_number = 1000\n",
    "\n",
    "    for i in data_size:\n",
    "        avg_iter, avg_err = run_experiment(i, run_number)\n",
    "        print(\"Dataset(N={0}): Average iterations:{1}(average error: {2}),\".format(\n",
    "            i, avg_iter, avg_err))\n",
    "\n",
    "```\n",
    "\n",
    "## 错误信息:\n",
    "\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"C:\\Users\\we\\Desktop\\perceptron.py\", line 103, in <module>\n",
    "    avg_iter, avg_err = run_experiment(i, run_number)\n",
    "  File \"C:\\Users\\we\\Desktop\\perceptron.py\", line 21, in run_experiment\n",
    "    iterations, final_weights = train(training_set, point_1, point_2)\n",
    "  File \"C:\\Users\\we\\Desktop\\perceptron.py\", line 67, in train\n",
    "    true_label = target_function(feature, point_1, point_2)\n",
    "  File \"C:\\Users\\we\\Desktop\\perceptron.py\", line 44, in target_function\n",
    "    x1, y1 = point_1\n",
    "NameError: global name 'point_1' is not defined\n",
    "[Finished in 0.259s]\n",
    "```\n",
    "\n",
    "这个就要涉及到 python 运行顺序问题. 暂且放在一边, 再学习python的时候, 碰到解决方案, 再回来尝试一下."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 代码参考\n",
    "\n",
    "https://github.com/blaenk/learning-from-data\n",
    "    \n",
    "\n",
    "之后可以继续简化代码, 使用 class 来封装.\n",
    "\n",
    "待定."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

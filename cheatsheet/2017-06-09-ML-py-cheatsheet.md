# Load data

- Load CSV files with the Python Standard Library
- Load CSV files with NumPy
- Load CSV files with Pandas

# Understand data with descriptive Statistics

- Peek 
- Dimensions
- Data Types
- Class Distribution
- Data summary 
- Correlations
- Skewness

# Data Visualization

- Univariate Plots
    - Histograms
    - Density plots
    - Box and Whisker plots
- Multivariate Plots

# Prepare Data

- Rescale data
- Standard data
- Normalize data
- Binarize data

# Feature Selection

- Univariate Selection
- Recursive Feature Elimination
- Principle Component Analysis
- Feature Importance

# Evaluate the performance of ML

- Hold-out: Train and test sets
- K-fold cross validation
- Leave one out cross validation
- Repeated random test-train splits
- Bootstrapping

WHEN:

1. Generally k-fold cross validation is the **gold standard** for evaluating the performance of a machine learning algorithm on unseen data with k set to 3, 5, or 10.
2. Using a train/test split is good for speed when using a slow algorithm and produces performance estimates with lower bias when using large datasets.
3. Techniques like leave-one-out cross validation and repeated random splits can be useful intermediates when trying to balance variance in the estimated performance, model training speed and dataset size.

# ML algorithm performance metrics

